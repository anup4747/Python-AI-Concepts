{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e3013c4-784a-4b5b-8595-208ae6209a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/anup/.local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: nltk in /home/anup/.local/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: gensim in /home/anup/.local/lib/python3.10/site-packages (4.3.3)\n",
      "Requirement already satisfied: filelock in /home/anup/.local/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/anup/.local/lib/python3.10/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/anup/.local/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/anup/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/anup/.local/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/anup/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/anup/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/anup/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/anup/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/anup/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/anup/.local/lib/python3.10/site-packages (from datasets) (0.34.3)\n",
      "Requirement already satisfied: packaging in /home/anup/.local/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/anup/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: click in /home/anup/.local/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/anup/.local/lib/python3.10/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/anup/.local/lib/python3.10/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/anup/.local/lib/python3.10/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/anup/.local/lib/python3.10/site-packages (from gensim) (7.3.0.post1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/anup/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/anup/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/anup/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/anup/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/anup/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/anup/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/anup/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/anup/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/anup/.local/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/anup/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/anup/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Requirement already satisfied: wrapt in /usr/lib/python3/dist-packages (from smart-open>=1.8.1->gensim) (1.13.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/anup/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/anup/.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets nltk gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebfbe771-12df-471b-b951-f93c20991a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anup/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/anup/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/anup/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/anup/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/anup/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "import string \n",
    "\n",
    "nltk.download('punkt')  # Run this once\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7210844a-3ba7-416e-bd89-93402da2fb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['personality', 'utterances'],\n",
      "        num_rows: 17878\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['personality', 'utterances'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "List(Value('string'))\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"AlekseyKorshuk/persona-chat\")\n",
    "print(ds)\n",
    "print(ds[\"train\"].features['personality'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0289ece7-a0dc-40b6-bcc2-0d12cea1da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_histories(dataset_split):\n",
    "    all_histories = []\n",
    "    for example in dataset_split:\n",
    "        for utter in example[\"utterances\"]:\n",
    "            all_histories.extend(utter[\"history\"])\n",
    "    return all_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a0e2d32-6638-48aa-9c75-4e97fe1dd2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_histories = extract_all_histories(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac7e9439-fce6-457d-b383-d889483a763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence  = [sent_tokenize(sentence) for sentence in train_histories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90166cec-33b2-4066-819b-26eedadf4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words  = [word_tokenize(word) for word in train_histories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "187b7380-9cbd-46b2-bd5a-c8a145303d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: ['hi , how are you doing ?', \"i'm getting ready to do some cheetah chasing to stay in shape .\"]\n",
      "Sample 2: ['hi , how are you doing ?', \"i'm getting ready to do some cheetah chasing to stay in shape .\"]\n",
      "Sample 3: ['you must be very fast .', 'hunting is one of my favorite hobbies .']\n",
      "Sample 4: ['i am !', 'for my hobby i like to do canning or some whittling .']\n",
      "Sample 5: ['hi , how are you doing ?', \"i'm getting ready to do some cheetah chasing to stay in shape .\"]\n",
      " \n",
      " \n",
      " \n",
      "Sample 1: ['hi', ',', 'how', 'are', 'you', 'doing', '?', 'i', \"'m\", 'getting', 'ready', 'to', 'do', 'some', 'cheetah', 'chasing', 'to', 'stay', 'in', 'shape', '.']\n",
      "Sample 2: ['hi', ',', 'how', 'are', 'you', 'doing', '?', 'i', \"'m\", 'getting', 'ready', 'to', 'do', 'some', 'cheetah', 'chasing', 'to', 'stay', 'in', 'shape', '.']\n",
      "Sample 3: ['you', 'must', 'be', 'very', 'fast', '.', 'hunting', 'is', 'one', 'of', 'my', 'favorite', 'hobbies', '.']\n",
      "Sample 4: ['i', 'am', '!', 'for', 'my', 'hobby', 'i', 'like', 'to', 'do', 'canning', 'or', 'some', 'whittling', '.']\n",
      "Sample 5: ['hi', ',', 'how', 'are', 'you', 'doing', '?', 'i', \"'m\", 'getting', 'ready', 'to', 'do', 'some', 'cheetah', 'chasing', 'to', 'stay', 'in', 'shape', '.']\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(tokenized_sentence[:5]):\n",
    "    print(f\"Sample {i+1}:\", item)\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "for i, item in enumerate(tokenized_words[:5]):\n",
    "    print(f\"Sample {i+1}:\", item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "999f0575-6971-4a02-970c-7592a340373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower words\n",
    "tokenized_words_lower = [[w.lower() for w in sentence] for sentence in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "732bd456-d432-4f55-b7e3-b90cccc62696",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "tokenized_words_lower_stemmer = [[stemmer.stem(w)  for w in sentence] for sentence in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c80a5cc-efcb-4e99-a6b3-cc3d7f1016a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For lemmatization:\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_words_lower_lemmatizer_stemmer = [[lemmatizer.lemmatize(w) for w in sentence] for sentence in tokenized_words_lower_stemmer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "917b7128-0ca1-4cd6-83ea-e78241f276d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: ['hi', ',', 'how', 'are', 'you', 'do', '?', 'i', \"'m\", 'get', 'readi', 'to', 'do', 'some', 'cheetah', 'chase', 'to', 'stay', 'in', 'shape', '.']\n",
      "Sample 2: ['hi', ',', 'how', 'are', 'you', 'do', '?', 'i', \"'m\", 'get', 'readi', 'to', 'do', 'some', 'cheetah', 'chase', 'to', 'stay', 'in', 'shape', '.']\n",
      "Sample 3: ['you', 'must', 'be', 'veri', 'fast', '.', 'hunt', 'is', 'one', 'of', 'my', 'favorit', 'hobbi', '.']\n",
      "Sample 4: ['i', 'am', '!', 'for', 'my', 'hobbi', 'i', 'like', 'to', 'do', 'canning', 'or', 'some', 'whittl', '.']\n",
      "Sample 5: ['hi', ',', 'how', 'are', 'you', 'do', '?', 'i', \"'m\", 'get', 'readi', 'to', 'do', 'some', 'cheetah', 'chase', 'to', 'stay', 'in', 'shape', '.']\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(tokenized_words_lower_lemmatizer_stemmer[:5]):\n",
    "    print(f\"Sample {i+1}:\", item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c80580d7-7a7a-4eef-ad97-0619f4c9a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.1120135e+00 -3.1371109e+00  9.8628002e-01 -1.4138199e+00\n",
      "  1.1013126e+00  4.4043180e-01  1.3829614e+00  4.9403863e+00\n",
      "  2.1753449e+00  6.8855494e-01  3.2491302e+00 -4.0629277e+00\n",
      "  1.5950819e+00 -4.1602191e-01 -1.3957081e+00 -6.0897164e-02\n",
      "  1.1386732e+00  2.5523132e-01 -2.3142867e+00 -7.6006442e-01\n",
      "  1.0086746e+00  1.2234819e+00  2.1721051e+00  8.6180019e-01\n",
      "  1.4680619e+00 -1.4477836e+00  3.1998506e+00 -1.8471208e+00\n",
      " -2.3887694e+00 -1.5174580e+00 -2.9299051e-01 -2.0039563e+00\n",
      "  1.9423991e+00 -3.1914544e+00 -2.9197788e+00  5.0887752e-02\n",
      "  5.6660414e-01 -8.5098046e-04 -1.5399626e+00  8.9345807e-01\n",
      " -1.1307522e+00  3.6456673e+00  1.6387032e+00  8.2336026e-01\n",
      "  4.7648218e-01  7.6938218e-01  5.6691134e-01 -1.6691879e+00\n",
      "  1.7558144e+00  1.4680330e+00  2.3486304e+00  1.6898100e+00\n",
      "  1.0102875e+00  1.1459527e+00 -1.7250291e+00 -1.1206555e+00\n",
      "  1.1402329e+00  6.0090929e-01  2.5346875e-01  3.3452353e+00\n",
      " -2.8080630e-01 -2.6340508e+00  1.7978719e+00  2.5745806e-01\n",
      " -1.9887897e+00  6.8348640e-01 -1.9299209e+00 -2.9931314e+00\n",
      " -1.7728344e-01 -5.2507743e-02  1.1124920e+00  1.3008549e+00\n",
      " -4.1496032e-01  3.4540713e-01 -1.4647158e+00 -1.2072868e-01\n",
      "  4.3844354e-01  1.1985254e+00  4.4082925e-02  9.8673671e-02\n",
      "  2.0872889e+00 -6.4388770e-01 -6.4156485e-01 -3.2362261e+00\n",
      " -1.5361699e+00 -6.9783741e-01  6.6245776e-01  1.4587605e+00\n",
      " -3.1127822e+00  8.1692278e-01 -3.0555241e+00  7.9451430e-01\n",
      "  1.9621794e+00  1.5662932e+00  3.5710540e-01  8.0999553e-01\n",
      " -2.4121654e+00 -1.2153841e+00  2.3807688e-01 -2.8664199e-01]\n",
      "Vector size: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenized_words example (list of token lists)\n",
    "tokenized_data = tokenized_words_lower_lemmatizer_stemmer\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Check the vector for a word\n",
    "word_vector = model.wv['hello']  # example word\n",
    "\n",
    "print(word_vector)\n",
    "print(\"Vector size:\", len(word_vector))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
